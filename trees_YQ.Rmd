---
title: "tree_YQ"
output: pdf_document
date: "2023-06-25"
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
#############################
# CHANGE DESCRIPTION
# I fit a different tree model with wage as y and black, south, urban, sibs, meduc, and feduc
# as x variables. Then I plot the tree, and add the text decisions. A 10-foldc cross-validation
# is performed then I found the size 5 generates lowest deviance. A linear regression model
# with similar x variables is computed,I notice that the mean sum of squared residuals was 
# 147318 using the linear model, compared to the residual mean deviance of 147400 using trees. 
# So the tree method has a little more residual error, which means it's a slightly worse predictor
# compared to linear model. A prediction is made by the two models with given x values. 
# Then I perform a random forest analysis with inlf as y and age, 
# educ, and wage as x. Using the "evaluate" function to calculate AUC (area under curve) 
# for this random forest model. 

# END OF CHANGE DESCRIPTION
#############################
# Instrumental Variables example
# install.packages("sem")
library(sem)
wages<-read.csv('http://inta.gatech.s3.amazonaws.com/wage2.csv')
iv.results<-tsls(lwage ~ educ + age + married + black, ~ feduc + age + married + black, data=wages)
# Run an instrumental variables regression. Note that father's education is
# used for an instrument, and is not in the first set of variables. Age,
# marital, status, and race are assumed to be less related to underlying
# ability and so are controlled for in the "first stage" regression too.

ols.results<-lm(lwage ~ educ + age + married + black, data=wages)
# Run the analogous OLS regression without father's education

print(summary(ols.results))
print(summary(iv.results))
# Note that the point estimate for education has now gone up. This suggests
# that people who have higher education in a way that's correlated with
# their father having higher education, even holding fixed age, marital
# status, and race, receive ~9% higher wages for every year of education
# Also note that the standard error on education is higher for the iv than
# ols results. The reason for this is that in IV, we're using only some of
# the variation in education: only the part related to father's education,
# so it's like there's less variation overall

# Stepwise regression and tree example
#install.packages(c('MASS','sem'))
library(MASS)
start.model<-lm(wage ~ hours + IQ + KWW + educ + exper + tenure + age + married + black + south + urban + sibs, data=wages)
# Give an initial model, which will be the most coefficients we'd want to ever use
summary(start.model)
stepwise.model<- step(start.model)
# The command "step" adds and subtracts coefficients to maximize a measure of
# goodness of fit, by default AIC
summary(stepwise.model)

# install.packages('tree')
library(tree)
wage.tree <- tree(wage ~ married + hours + IQ + KWW + educ + tenure + exper, method="anova", data=wages)
# fit a tree
summary(wage.tree)
# Print the results

plot(wage.tree)
text(wage.tree)
# Plot the tree, and add the text decisions

cross.validation <- cv.tree(wage.tree)
# perform cross-validation, 10-fold. 

cross.validation
# look for the size with the lowest "dev" or deviance
# Why might this be different than the fitted tree?


#The key difference between cross-validation and a fitted tree is their purpose 
#and scope.Cross-validation assesses the generalization performance of a model 
# by simulating its performance #on unseen data, whereas a fitted tree is a 
# specific model trained on a given dataset and is ready 
# to make predictions on new data. Cross-validation helps evaluate and compare 
# different models or #tuning parameters, whereas a fitted tree is a single 
# model that has learned patterns and relationships in the training data.


pruned.wage.tree<-prune.tree(wage.tree,best=5)
# Prune this tree down to 5 terminal nodes, just to show this is how you would
# do it. Here this makes it worse though
summary(pruned.wage.tree)

##### Code change

wage.tree2 <- tree(wage ~ black + south + urban + sibs + meduc + feduc, method="anova", data=wages)
# fit a tree
summary(wage.tree2)
plot(wage.tree2)
text(wage.tree2)
cross.validation2 <- cv.tree(wage.tree2)
# perform cross-validation, 10-fold. 
cross.validation2
# look for the size with the lowest "dev" or deviance

#########
lmfit<- lm(wage ~ married + hours + IQ + KWW + educ + tenure + exper, data=wages)
# Compute a regression using those same variables:
anova(lmfit)
summary(wage.tree)
# Notice that the mean sum of squared residuals was 130895 using the linear
# model, compared to the residual mean deviance of 130000 using trees. So the
# tree method has less residual error, which means it's a better predictor.
# This is especially striking given that the tree only uses KWW, educ, IQ,
# While the regression needs substantial contributions from tenure, experience,
# and marital status too.

predict(wage.tree, newdata=data.frame(IQ=100, KWW=50, educ=16, married=1, hours=40, tenure=3, exper=2))
predict(lmfit, newdata=data.frame(IQ=100, KWW=50, educ=16, married=1, hours=40, tenure=3, exper=2))
# These predictors give different estimates for the expected wage of a
# particular individual, relatively young, married, well educated, with good
# knowledge of the world of work. Which would you trust and why?

# I would trust tree as it is better predictor compared to linear method.


# Extension: try to build another tree, and see what it looks like. 
# You can sample the data with the command:
# install.packages('dplyr')
# library('dplyr')
# wages.sample <- sample(wages, n=500) 

#### Code change
lmfit2<- lm(wage ~ black + south + urban + sibs + meduc + feduc, data=wages)
# Compute a regression using those same variables:
anova(lmfit2)
summary(wage.tree2)
# Notice that the mean sum of squared residuals was 147318 using the linear
# model, compared to the residual mean deviance of 147400 using trees. So the
# tree method has a little more residual error, which means it's a worse predictor
# compared to linear model.
predict(wage.tree2, newdata=data.frame(black=0, south=0, urban=0, sibs=2, meduc=8, feduc=8))
predict(lmfit2, newdata=data.frame(black=0, south=0, urban=0, sibs=2, meduc=8, feduc=8))
# These predictors give different estimates for the expected wage of a
# particular individual.
##########

# Takehome exercise from last class
lfp <- read.csv('http://inta.gatech.s3.amazonaws.com/mroz_train.csv')
lfp$inlf<-as.factor(lfp$inlf) # We need the outcome variable to be a 'factor'

inlf.tree <-tree(as.numeric(inlf) - 1 ~huseduc + husage + kidslt6 + kidsge6 + nwifeinc + educ + age, data=lfp, na.action=na.omit) # Fit the model
inlf.lm <- lm(as.numeric(inlf) - 1 ~huseduc + husage + kidslt6 + kidsge6 + nwifeinc + educ + age, data=lfp) # Fit the model
print(inlf.tree)
# compute predictions manually, and save them as lfp$predicted.inlf
lfp$predicted.inlf<-predict(inlf.tree)


library(pROC)
evaluate <- function(y_true, y_predicted, detail=FALSE) {
  curve<-roc(y_true, y_predicted)
  if (detail) {
  print(ci.auc(curve))
  print('Sensitivity (True Positive Rate)')
  tpr<-ci.se(curve)
  print(tpr)
  print('Specificity (True Negative Rate)')
  tnr<-ci.sp(curve)
  print(tnr)
  }
  cat('Point estimate (final word on effectiveness)\n')
  print(auc(curve))
  #print('Point estimate of AUCROC: ' + auc(curve))
}
evaluate(lfp$inlf, lfp$predicted.inlf)


library(randomForest)
lfp <- read.csv('http://inta.gatech.s3.amazonaws.com/mroz_train.csv')
lfp[is.na(lfp)] <- 0
rf <-randomForest(as.factor(inlf) ~ hours  +  kidslt6  , data=lfp)
evaluate(as.factor(lfp$inlf), predict(rf,type="prob")[,1])

#####code change
rf2 <-randomForest(as.factor(inlf) ~ age  +  educ + wage  , data=lfp)
evaluate(as.factor(lfp$inlf), predict(rf2,type="prob")[,1])
#########

lfp.out <- read.csv('http://inta.gatech.s3.amazonaws.com/mroz_test.csv')
lfp.out[is.na(lfp.out)] <- 0
evaluate(as.factor(lfp.out$inlf), predict(rf, newdata=lfp.out, type="prob")[,1])
evaluate(lfp.out$inlf, predict(inlf.lm, newdata=lfp.out))
evaluate(lfp.out$inlf, predict(inlf.tree, newdata=lfp.out))

```

